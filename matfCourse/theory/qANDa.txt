Vim folding brush-up:
:set foldmethod=indent   // sakrij sve odgovore
:set foldmethod=syntax   // prikaži sve odgovore

za = toogle fold
zo = fold open
zc = fold close



===========================================================================
===========================================================================
# 1 Uvod
===========================================================================
===========================================================================

1.1 Šta je mašinsko učenje, opisati svojim rečima?
    -
    -

1.2 Koje su osnovne grupe problema mašinskog učenja?
    -
    -

1.3 Opisati nadgledano učenje?
    -
    -

1.4 Opisati nenadgledano učenje?
    -
    -

1.5 Opisati učenje uslovljavanjem?
    -
    -

===========================================================================
===========================================================================
# 2. Nadgledano učenje
===========================================================================
===========================================================================

2.0.1 Šta je model?
    -
    - Model je funkcija koja kao ulaz prima vrednosti atributa, a kao izlaz vraća vrednosti ciljne promenljive.

2.0.2 Šta predstavlja generalizacija?
    -
    - Generalizacija predstavlja sposobnost modela da na osnovu proizvoljih vrednosti atributa dobro proceni ciljnu promenljivu.

2.0.3 Koje su osnovne vrste problema nadgledanog učenja?
    -
    - Regresija i klasifikacija.

2.0.4 Šta je regresija?
    -
    - Regresija predstavlja problem nadgledanog učenja sa neprekidnom ciljnom promenljivom.

2.0.5 Šta je klasifikacija?
    -
    - Regresija predstavlja problem nadgledanog učenja sa kategoričkom ciljnom promenljivom.

2.0.6 Šta su kategoričke promenljive?
    -
    - Kategoričke promenljive imaju konačan broj različitih vrednosti koje mogu da uzmu, i ne mogu se urediti.

===========================================================================
## 2.1 Postavka problema nadgledanog učenja
===========================================================================

2.1.1 Čime je dat odnos između atributa i ciljne promenljive?
    -
    - Odnos između atributa i ciljne promenljive dat je zajedničkom raspodelom verovatnoća.

2.1.2 Šta se koristi kao zamena za zajedničku raspodelu verovatnoće atributa i ciljne promenljive?
    -
    - Model f(x), koji vrednostima atributa dodeljuje vrednosti ciljne promenljive.

2.1.3 Šta predstavlja kvalitet modela?
    -
    - Kvalitet modela predstavlja koliko dobro procenjuje vrednosti ciljne promenljive.

2.1.4 Šta je funkcija greške?
    -
    - Funkcija greške predstavlja razliku između ciljne promenljive i procene ciljne promenljive date od strane modela po nekoj metrici.

2.1.5 Šta je funkcional rizika ili stvarni rizik? Navesti formulu u opštem slučaju.
    -
    -

2.1.6 Iz kojih razloga je nemoguće direktno rešiti problem nadgledanog učenja u svom teorijskom obliku?
    -
    - Ne znamo zajedniču raspodelu verovatnoća atributa i ciljne promenljive. Skup funkcija po kojem minimizujemu stvarni rizik je beskonačan.

===========================================================================
## 2.2 Princip minimizacije empirijskog rizika
===========================================================================

2.2.1 Skup funkcija po kojem se može minimizovati stvarni rizik pri nadgledanom učenju je beskonačan, kako se ovaj problem rešava?
    -
    - Uzimamo konačan skup koji predstavljamo reprezentacijom modela, po kojem minimizujemo rizik.

2.2.2 Šta predstavlja reprezentacija modela? Navesti neki primer.
    -
    - Reprezentacija modela predstavlja skup različitih modela koji mogu biti izraženi.

2.2.3 Kako se rešava problem nedostupnosti gustine raspodele p(x,y) kod nadgledanog učenja?
    -
    - Gustinu raspodele aproksimiramo na osnovu uzorka.

2.2.4 Koja je razlika između stvarnog rizika i empirijskog rizika?
    -
    - Empirijski rizik predstavlja procenu stvarnog rizika.

2.2.5 Da li funkcija koja minimizuje emprijiski rizik u svakom slučaju dobro aproksimira funkciju koja minimizuje stvarni rizik? Obrazložiti odgovor.
    -
    - Ne, funkcija f se može se ponašati super na tačkama na kojima uči, ali pritom loše generalizovati. Ova pojava se naziva preprilagođavanje.
    - primer - karakteristična funkcija, svuda 0, osim u tačkama uzorka, gde je baš vrednost ciljne promenljive.

2.2.6 Kako glasi regresiona funkcija?
    -
    - r(x) = E(y|x) = Integral( y*p(y|x) dy )

2.2.7 Navesti funkciju greške kod regresije koja se najčešće koristi.
    -
    - L(u,v) = (u-v)^2
    - kvadratna greška

2.2.8 Kako glasi formulacija principa minimizacije empirijskog rizika za regresiju?
    -
    - funkcija koja minimizuje empirijski rizik se uzima za aproksimaciju funkcije koja minimizuje stvarni rizik.

2.2.9 Da li funkcija greške mora biti simetrična? Obrazložiti.
    -
    - Ne mora, ako želimo na različit način da kaznimo različite vrste grešaka.

2.2.10 Navesti funkciju greške kod klasifikacije.
    -
    - L(u|v) = I(u=/=v)

===========================================================================
## 2.3 Preprilagođavanje
===========================================================================

2.3.1 Objasniti pojam preprilagođavanja.
    -
    - Preprilagođavanje predstavlja prilagođavanje modela podacima u toj meri, da se gubi moć generalizacije.
    - Za model kažemo da preprilagođava ako pravi malu grešku na trening skupu, a veliku grešku na podacima koji do tada nisu viđeni, što takođe znači da model loše generalizuje.

2.3.2 Da li je minimizacija srednje greške jedini i najbolji kriterijum za procenu kvaliteta modela? Obrazložiti odgovor.
    -
    - Zavisi na kojim podacima se vrši. Ako procenjujemo grešku nad podacima nad kojim učimo, preprilagođavanje podacima može uticati na ocenu.

2.3.3 Koji sve mogu biti razlozi preprilagođavanja?
    -
    - Preprilagođavanje može biti posledica učenja šuma trening skupa, ili učenja specifičnosti podataka koji nisu suštinski bitni za određivanje ciljne promenljive.

2.3.4 Kako se rešava problem preprilagođavanja?
    -
    - Uvođenjem regularizacionog izraza, time menjajući model koji se uči.

===========================================================================
## 2.4 Regularizacija
===========================================================================

2.4.1 Šta predstavlja regularizacija?
    -
    - Preprilagođavanje predstavlja bilo kakav vid smanjivanja fleksibilnosti modela, u pokušaju da se smanji preprilagođavanje a poveća mogućnost generalizovanja modela.

2.4.2 Čemu služi regularizacioni parametar lambda?
    -
    - Služi da pridodamo važnost regularizacionom izrazu u odnosu na minimizaciju srednje greške.

2.4.3 Navedite primer regularizacionog izraza.
    -
    - Lnorme, laso(menhetn), euklidsko

2.4.4 Kako na model utiče regularizacioni parametar u svojim ekstremnim vrednostima.
    -
    - kad je 0, uopšte ne utiče, tako da ako je model dovoljno fleksibilan može doći do preprilagođavanja.
    - ako teži beskonačnosti, samo on i utiče na minimizaciju, tako da model u potpunosti gubi moć prilagođavanja, jedino mu je bitno da svi parametri imaju vrednosti 0.

2.4.5 Da li regularizacija može loše uticati na naš model? Obrazložiti.
    -
    - Dap, ako previše regularizujemo model u potpunosti gubi mogućnost prilagođavanja.

2.4.6 Da li regularizacija kod velike količine podataka dobro ili loše utiče na model?
    -
    - Loše, regularizacija u ovom slučaju onemogućuje modelu da se dovoljno prilagodi podacima.

===========================================================================
## 2.5 Nagodba između sistematskog odstupanja i varijanse
===========================================================================

2.5.1 Šta je sistematsko odstupanje
    -
    - Sistematsko odstupanje je razlika između onoga što treba oceniti i očekivanja onoga čime se ocena vrši.
    - Sistematsko odstupanje je konstantno odstupanje procenjene ciljne promenljive od njene stvarne vrednosti, koju model pravi nezavisno od uzorka nad kojim uči.
    - Veliko sistematsko odstupanje je dobar znak da je došlo do lošeg prilagođavanja(underfitting)

2.5.2 Šta je varijansa
    -
    - Varijansa predstavlja grešku zbog osetljivosti modela na male promene u uzorku nad kojim uči.
    - Varijansa kvantifikuje koliko se model može razlikovati u zavisnosti od uzorka nad kojim uči.
    - Velika varijansa je dobar znak da je došlo do preprilagođavanja(overfitting)


2.5.3 Kako možemo uticati na fleksibilnost modela?
    -
    - Regularizacijom, tačnije menjanjem regularizacionog parametra.

2.5.4 Šta predstavlja nagodba između sistematsog odstupanja i varijanse?
    -
    - Smanjivanje varijanse i pritom povećavanje sistematskog odstupanja u zavisnosti od izabrane vrednosti regularizacionog parametra, čime smanjujemo preprilagođavanje modela i povećavamo njegovu moć generalizacije.

===========================================================================
## 2.6 Teorijske garancije kvaliteta garanije
===========================================================================

2.6.1 Čime se bavi statistička teorija učenja
    -
    - STU se bavi proučavanjem moći generalizacije različitih skupova modela.

2.6.2 Koja su osnovna pitanja koja se postavljaju u statičkoj teoriji učenja?
    -
    - Koliki može biti stvarni rizik modela, ako znamo njegov empirijski rizik.
    - Koliko je stvarni rizik modela f viši od stvarnog rizika najboljeg modela iz nekog skupa modela.
    - Koliko je stvarni rizik modela f viši od stvarnog rizika najboljeg mogućeg modela.

2.6.3 Od čega sve zavisi gornja granica stvarnog rizika u odnosu na neki model f?
    -
    - Od empirijskog rizika modela f, veličine skupa podataka i skupa modela iz kojeg se bira f.

2.6.4 Šta nam govori hefdingova nejednakost?
    -
    - Hefdingova nejednakost nam kvantifikuje brzinu konvergencije empirijskog rizika nekog modela ka njegovom stvarnom riziku.

2.6.5 Šta predstavlja funkcija rasta?
    -
    - Funkcija rasta je maksimalan broj načina da funkcije iz skupa F klasifikuju N tačaka.


2.6.6 Šta predstavlja Vapnik-Červonenkisova dimenzija skupa funkcija F?
    -
    - VC dimenzija skupa funkcija F je najveći broj N, za koji postoji skup tačaka veličine N koje se mogu klasifikovati pomoću skupa funkcija F na sve moguće načine.

2.6.7 Koja je VC dimenzija skupa svih pravih u prostoru?
    -
    - 2, ako se tačka nalazi na pravoj onda pripada jednoj klasi, drugoj u suprotnom.

2.6.8 Dati primer skupa funkcija koji ima beskonačnu VC dimenziju.
    -
    - {sgn(sin(wx)) | w in R}
    - SVM sa gausovim kernelom

2.6.9 Kakav treba da bude skup modela, da bi dovoljno velik broj podataka garantovao poverenje u empirijsku ocenu stvarnog rizika?
    -
    - Skup modela treba biti konačne VC dimenzije.

===========================================================================
## 2.7 Veza statističke teorije učenja sa filozofijom nauke
===========================================================================

2.7.1 Koje je najbolje objašnjenje neke pojave u smislu Okamove oštrice?
    -
    - Ono koje pored objašnjenja te pojave, objašnjava što manje.

===========================================================================
## 2.8 Vrste modela
===========================================================================

2.8.1 Koje vrste modela postoje, prema tome koliku količinu informacije pokušavaju da modeluju?
    -
    - Probabilistički generativni
    - Probabilistički diskriminativni
    - Neprobabilistički diskriminativni

2.8.2 Šta pokušavaju da modeluju probabilistički generativni modeli?
    -
    - Zajedničku raspodelu datu gustinom p(x,y), koja opisuje zavisnosti između atributa i ciljne promenljive ali i međuzavisnosti između atributa.
    - Mogu se vršiti predviđanja, izračunati pouzdanost predviđanja, generacija instanci.

2.8.3 Šta pokušavaju da modeluju probabilistički diskriminativni modeli?
    -
    - Uslovnu raspodelu datu gustinom p(y|x), opisuje samo zavisnost ciljne promeljive od atributa. Takođe se može proceniti pouzdanost predviđanja uz naravno predviđanje ciljne promenljive.

2.8.4 Šta pokušavaju da modeluju neprobabilistički diskriminativni modeli?
    -
    - Funkciju f(x) = y, kojom se mogu vršiti samo predviđanja ciljne promenljive, bez procene pouzdanosti datog predviđanja.


2.8.5 Koje su dimenzije dizajna algoritama nadgledanog učenja?
    -
    - Vrsta modela
    - Forma modela
    - Funkcija greške
    - Regularizacija
    - Optimizacioni algoritam


===========================================================================
===========================================================================
# 3 Probabilistički modeli
===========================================================================
===========================================================================

===========================================================================
## 3.1 Linearna regresija
===========================================================================

3.1.1 Koja je osnovna pretpostavka linearne regresije sa probabilističkog gledišta?
    -
    - Osnovna pretpostavka je da se funkcija raspodele ciljne promenljive za date atribute može aproksimirati normalnom raspodelom.

3.1.2 Da li je uvek pametno verovati oceni greške linearne regresiji, ako ne zašto?
    -
    - Ne, zbog potencijalnog preprilagođavanja.

3.1.3 Koji su problemi sa algebarskim rešenjem problema linearne regresije?
    -
    - Matrica ne transpose(X)*X nije nužno invertabilna, čak i ako jeste, moguće je da je loše uslovljena. Još veći problem je što inverz matrice može biti izuzetno skup, posebno za visokodimenzionalne podatke sa velikim brojem instanci.

3.1.4 Kako se rešava problem neinvertabilnosti i loše uslovljenosti matrice transpose(X)*X kod algebarskog rešenja linearne regresije?
    -
    - Regularizacijom.

3.1.5 Kako se rešava problem izuzetno skupe inverzije za visokodimenzionalnu matricu transpose(X)*X kod algebarskog rešenja linearne regresije?
    -
    - Prelaskom na gradijentni spust.

3.1.6 Koji su preduslovi da bi linearan model bio intepretabilan
    -
    - model bi trebalo da ima malu grešku
    - podaci bi trebalo da budu standardizovani, kako bi bilo lakše uočiti koliko koji atribut utiče

3.1.7 Šta je binarno kodiranje(dummy codding)?
    - Binarnim kodiranjem se kategoričke promenljive transformišu u numeričke. Za C različitih kategorija se pravi C-1 binarna promenljiva, svakoj kategoriji odgovara različita kombinacija. Za svaku kombinaciju važi da u sebi sadrži najviše jednu jedinicu.

3.1.8 Zašto se pri binarnom kodiranju jedna kategorija definiše na specijalan način, sa svim nulama, a ne kao još jedna dodatna promenljiva?
    -
    - U tom slučaju bi svuda postojala tačno jedna jedinica, pa bi te kolone bile zavisne sa slobodnim članom koji se gleda kao kolona jedinica koja se množi parametrom, što vodi neinvertabilnosti matrice transpose(X)*X.

3.1.9 Koje su mane kod linearne regresije?
    -
    - Pretpostavka nezavisnosti atributa, pretpostavka konstantne varijanse, potencijalna neadekvatnost linearne forme, preveliki uticaj odudarajućih vrednosti(outliera).

3.1.10 Koje su prednosti linearne regresije?
    -
    - Jednostavnost modela, brzina treniranja, interpretabilnost.

===========================================================================
## 3.2 Logistička regresija
===========================================================================

3.1.1 Koja je osnovna pretpostavka logističke regresije sa probabilističkog gledišta?
    -
    - Osnovna pretpostavka je da se funkcija raspodele ciljne promenljive za date atribute može aproksimirati bernulijevom raspodelom.


3.1.2 Zašto se ne može koristiti linearni model zavisnosti između parametra h i vrednosti atributa x kod logističke regresije?
    -
    - Zato što parametar h modelira verovatnoću, tako da se mora nalaziti u intervalu od 0 do 1, dok x pripada intervalu od -inf do inf.

3.1.3 Kako glasi sigmoidna funkcija?
    -
    - sig(t) = 1/( 1 + exp(-t) )


3.1.4 Kako glasi funkcija greške unakrsna entropija?
    -
    - L(u, v) = -u*log(v) - (1-u)*log(1-v)


===========================================================================
## 3.3 Multinomijalna logistička regresija
===========================================================================


===========================================================================
## 3.4 Uopšteni linearni modeli
===========================================================================


===========================================================================
## 3.5 Naivni Bajesov algoritam
===========================================================================


===========================================================================
===========================================================================
# 4 Modeli zasnovani na širokom pojasu
===========================================================================
===========================================================================


===========================================================================
===========================================================================
# 6 Ansambli
===========================================================================
===========================================================================


===========================================================================
===========================================================================
# 5 Modeli zasnovani na instancama
===========================================================================
===========================================================================


===========================================================================
===========================================================================
# 7 Evaluacija i izbor modela
===========================================================================
===========================================================================


===========================================================================
===========================================================================
# 8 Regularizacija
===========================================================================
===========================================================================


===========================================================================
===========================================================================
# 9 Optimizacija
===========================================================================
===========================================================================

8.0.1 Čime se bavi matematička optimizacija

8.0.2 Opisati robins-moorove uslove za izbor duzine koraka kod gradijentnog spusta.

8.0.2 Navesti i objasniti najčešće kriterijume zaustavljanja kod gradijentnog spusta.



===========================================================================
===========================================================================
# 10 Neuronske mreže i duboko učenje
===========================================================================
===========================================================================


===========================================================================
===========================================================================
# 11 Šta ako ne radi?
===========================================================================
===========================================================================


===========================================================================
===========================================================================
# 12 Markovljevi procesi odlučivanja i njihovo rešavanje
===========================================================================
===========================================================================


===========================================================================
===========================================================================
# 13 Učenje u nepoznatom okruženju
===========================================================================
===========================================================================


===========================================================================
===========================================================================
# 14 Matematičko predznanje
===========================================================================
===========================================================================


